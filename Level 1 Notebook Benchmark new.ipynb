{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626a022d-5469-4e2c-aba7-c6b00be60b54",
   "metadata": {},
   "source": [
    "# Level 1: Rice Crop Discovery Tool Benchmark Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd5ecb-92da-4356-860f-64cec7534b45",
   "metadata": {},
   "source": [
    "## Challenge Level 1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e854b5f4-ee16-40da-a6d1-81898cc52aae",
   "metadata": {},
   "source": [
    "<p align=\"justify\">Welcome to the EY Open Science Data Challenge 2023! This challenge consists of two levels – Level 1 and Level 2. This is the Level 1 challenge aimed at participants who are beginners or have intermediate skill sets in data science and programming. The goal of Level 1 is to predict the presence of rice crops at a given location using satellite data. By the time you complete this level, you will have developed a rice crop classification model, which can distinguish between rice and non-rice fields. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c6226-0341-48c1-82b6-3fb000064462",
   "metadata": {},
   "source": [
    "<b>Challenge Aim: </b><p align=\"justify\"> <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd1b29",
   "metadata": {},
   "source": [
    "<p align=\"justify\">In this notebook, we will demonstrate a basic model workflow that can serve as a starting point for the challenge. The basic model has been built to predict rice crops against non-rice crops (which might include forest, other vegetation and water bodies) using features from the Sentinel-1 Radiometrically Terrain Corrected (RTC)  dataset as predictor variables. In this demonstration, we have used two features from the Sentinel-1 dataset, namely VV (Vertical polarization – Vertical polarization) and VH (Vertical polarization – Horizontal polarization) and trained a logistic regression model with these features. We have extracted the VV band and VH band data from the Sentinel-1 dataset for one day (21st March 2020), with an assumption that VV and VH values for this day are representative of VV and VH values for the entire year (2020) for a given location.\n",
    "\n",
    "Most of the functions presented in this notebook were adapted from the <a href=\"https://planetarycomputer.microsoft.com/dataset/sentinel-1-rtc#Example-Notebook\">Sentinel-1-RTC notebook</a> found in the Planetary Computer portal.</p>\n",
    "    \n",
    "<p align=\"justify\"> Please note that this notebook is just a starting point. We have made many assumptions in this notebook that you may think are not best for solving the challenge effectively. You are encouraged to modify these functions, rewrite them, or try an entirely new approach.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb152d6-21e5-46c6-931d-11e99e6a6798",
   "metadata": {},
   "source": [
    "## Load In Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093dca36-dae9-4b87-9026-7508740cd746",
   "metadata": {},
   "source": [
    "To run this demonstration notebook, you will need to have the following packages imported below installed. This may take some time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546caac-b5a5-494f-95e8-19a01e117492",
   "metadata": {},
   "source": [
    "#### Note: Environment setup\n",
    "Running this notebook requires an API key.\n",
    "\n",
    "To use your API key locally, set the environment variable <i><b>PC_SDK_SUBSCRIPTION_KEY</i></b> or use <i><b>planetary_computer.settings.set_subscription_key(<YOUR API Key>)</i></b><br>\n",
    "See <a href=\"https://planetarycomputer.microsoft.com/docs/concepts/sas/#when-an-account-is-needed\">when an account is needed for more </a>, and <a href=\"https://planetarycomputer.microsoft.com/account/request\">request</a> an account if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a03723e-78ae-4150-ba22-e2e485b95cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import ipyleaflet\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score,classification_report,confusion_matrix\n",
    "\n",
    "# Planetary Computer Tools\n",
    "import pystac\n",
    "import pystac_client\n",
    "import odc\n",
    "from pystac_client import Client\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "from odc.stac import stac_load\n",
    "import planetary_computer as pc\n",
    "pc.settings.set_subscription_key('c861000c00fb430494b6ced2d9b15cf3')\n",
    "\n",
    "# Others\n",
    "import requests\n",
    "import rich.table\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c268cf6",
   "metadata": {},
   "source": [
    "## Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80dbf04",
   "metadata": {},
   "source": [
    "Before building the model, we need to load in the rice crop presence data. We have curated for you data from a certain region in Vietnam for the year 2020. The data consists of  geo locations (Latitude and Longitude) with a tag specifying if the crop present in each geo location is rice or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1da678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude and Longitude</th>\n",
       "      <th>Class of Land</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(10.323727047081501, 105.2516346045924)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(10.322364360592521, 105.27843410554115)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(10.321455902933202, 105.25254306225168)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(10.324181275911162, 105.25118037576274)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(10.324635504740822, 105.27389181724476)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Latitude and Longitude Class of Land\n",
       "0   (10.323727047081501, 105.2516346045924)          Rice\n",
       "1  (10.322364360592521, 105.27843410554115)          Rice\n",
       "2  (10.321455902933202, 105.25254306225168)          Rice\n",
       "3  (10.324181275911162, 105.25118037576274)          Rice\n",
       "4  (10.324635504740822, 105.27389181724476)          Rice"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_presence_data = pd.read_csv(\"Crop_Location_Data.csv\")\n",
    "crop_presence_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6812c-7137-4873-b4ed-2dcdd470209b",
   "metadata": {},
   "source": [
    "## Predictor Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487a9dc-1308-4c05-a69a-ccfe60bc9100",
   "metadata": {},
   "source": [
    "<p align =\"justify\">Now that we have our crop location data, it is time to gather the predictor variables from the Sentinel-1 dataset. For a more in-depth look regarding the Sentinel-1 dataset and how to query it, see the Sentinel-1 <a href=\"https://challenge.ey.com/api/v1/storage/admin-files/6403146221623637-63ca8d537b1fe300146c79d0-Sentinel%201%20Phenology.ipynb/\"> supplementary \n",
    "notebook</a>.\n",
    "    \n",
    "\n",
    "<p align = \"justify\">Sentinel-1 radar data penetrates through the clouds, thus helping us to get the band values with minimal atmospheric attenuation. Band values such as VV and VH help us in distinguishing between the rice and non rice crops. Hence we are choosing VV and VH as predictor variables for this experiment. \n",
    "        \n",
    "<ul>\n",
    "<li>VV - gamma naught values of signal transmitted with vertical polarization and received with vertical polarization with radiometric terrain correction applied.\n",
    "\n",
    "<li>VH - gamma naught values of signal transmitted with vertical polarization and received with horizontal polarization with radiometric terrain correction applied.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04011667-99ae-4820-a635-d8d50f716fe3",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 1</strong></h4>\n",
    "<p align=\"justify\">Participants might explore other combinations of bands from the Sentinel-1 data. For example, you can use mathematical combinations of bands to generate various <a href=\"https://challenge.ey.com/api/v1/storage/admin-files/3868217534768359-63ca8dc8aea56e00146e3489-Comprehensive%20Guide%20-%20Satellite%20Data.docx\">vegetation indices </a> which can then be used as features in your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85257f-4a48-49e8-8036-10e9a6b69894",
   "metadata": {},
   "source": [
    "### Accessing the Sentinel-1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399737c-46bb-44b7-bda8-4253c827e66d",
   "metadata": {},
   "source": [
    "<p align = \"Justify\">To get the Sentinel-1 data, we write a function called <i><b>get_sentinel_data.</b></i> This function will fetch VV and VH band values for a particular location over the specified time window. In this example, we have extracted VV and VH values for a day (21st March 2020). </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f85d6d-6c72-438b-81f8-8aafb1265b0a",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 2</strong></h4>\n",
    "<p align=\"justify\"> Extract VV and VH band values for an entire year. Different land classes (e.g., agriculture, water, urban) will have different annual variability. This variability will be better than a single date for accurately identifying land classes. Please find below a demonstration of extracting data for a day (21st March 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e339a34-cfa7-4899-9165-63ca7c01bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentinel_data(latlong,time_slice,assets):\n",
    "    '''\n",
    "    Returns VV and VH values for a given latitude and longitude \n",
    "    Attributes:\n",
    "    latlong - A tuple with 2 elements - latitude and longitude\n",
    "    time_slice - Timeframe for which the VV and VH values have to be extracted\n",
    "    assets - A list of bands to be extracted\n",
    "    '''\n",
    "\n",
    "    latlong=latlong.replace('(','').replace(')','').replace(' ','').split(',')\n",
    "    bbox_of_interest = (float(latlong[1]) , float(latlong[0]), float(latlong[1]) , float(latlong[0]))\n",
    "    time_of_interest = time_slice\n",
    "\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "    )\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-1-rtc\"], bbox=bbox_of_interest, datetime=time_of_interest\n",
    "    )\n",
    "    items = list(search.get_all_items())\n",
    "    bands_of_interest = assests\n",
    "    data = stac_load([items[0]], patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
    "    vh = data[\"vh\"].astype(\"float\").values.tolist()[0][0]\n",
    "    vv = data[\"vv\"].astype(\"float\").values.tolist()[0][0]\n",
    "    return vh,vv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab9734-6cdf-466f-87c3-8067b05b90ba",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 3 </strong></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb9b25-0c41-4f55-aa33-082adeb34dbd",
   "metadata": {},
   "source": [
    "Explore the approach of building a bounding box (e.g., 5x5 pixels) around the given latitude and longitude positions and then extract the aggregated band values (e.g., average, median) to get normalized band values to build the model. Radar data has inherent variability at the pixel level due to variable scattering response from the target. This effect is called “speckle” and it is common to filter the data to smooth these variations. Try using a 3x3, 5x5 or 7x7 window around the specific latitude and longitude point to get improved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c51cd6e-41e2-4df4-ae07-349be861f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Function call to extract VV,VH Values\n",
    "# time_slice = \"2020-03-20/2020-03-21\"\n",
    "# assests = ['vh','vv']\n",
    "# vh_vv = []\n",
    "# for coordinates in tqdm(crop_presence_data['Latitude and Longitude']):\n",
    "#     vh_vv.append(get_sentinel_data(coordinates,time_slice,assests))\n",
    "# vh_vv_data = pd.DataFrame(vh_vv,columns =['vh','vv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa872546-7567-418c-af04-1d8b4fa5bd60",
   "metadata": {},
   "source": [
    "## Joining the predictor variables and response variables\n",
    "Now that we have extracted our predictor variables, we need to join them onto the response variable . We use the function <i><b>combine_two_datasets</b></i> to combine the predictor variables and response variables.The <i><b>concat</b></i> function from pandas comes in handy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96296d95-8290-4f26-80f9-9e221bfcfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_two_datasets(dataset1,dataset2):\n",
    "#     '''\n",
    "#     Returns a  vertically concatenated dataset.\n",
    "#     Attributes:\n",
    "#     dataset1 - Dataset 1 to be combined \n",
    "#     dataset2 - Dataset 2 to be combined\n",
    "#     '''\n",
    "#     data = pd.concat([dataset1,dataset2], axis=1)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20fa2b5f-727b-4781-9ff4-cc70596cd3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_data = combine_two_datasets(crop_presence_data,vh_vv_data)\n",
    "# crop_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4af7b5-41d1-4822-8d78-5e4bdc84b287",
   "metadata": {},
   "source": [
    "## Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664ca55-dba1-440b-a4ef-2b6934c03929",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> Now let us select the columns required for our model building exercise. We will consider only VV and VH for our model. It does not make sense to use latitude and longitude as predictor variables as they do not have any impact on presence of rice crop.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "536d53ad-2697-4283-8ceb-db94f93bad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_data = crop_data[['vh','vv','Class of Land']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca40658-8003-4195-b0d1-5efe29929ac9",
   "metadata": {},
   "source": [
    "### Train and Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46507f5-cfd1-43d8-8b05-aa09ce8016af",
   "metadata": {},
   "source": [
    "<p align=\"justify\">We will now split the data into 70% training data and 30% test data. Scikit-learn alias “sklearn” is a robust library for machine learning in Python. The scikit-learn library has a <i><b>model_selection</b></i> module in which there is a splitting function <i><b>train_test_split</b></i>. You can use the same.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "062ac03d-245d-49af-82d9-56d3ac581e85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crop_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2r/8n96nprj6xj63r26db8sxy640000gn/T/ipykernel_51764/2230858858.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class of Land'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_data\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Class of Land'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crop_data' is not defined"
     ]
    }
   ],
   "source": [
    "X = crop_data.drop(columns=['Class of Land']).values\n",
    "y = crop_data ['Class of Land'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y,random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031607b-9e71-46c8-beb1-3610ab59c56a",
   "metadata": {},
   "source": [
    "### Feature Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2140c8-3fed-49a4-b9c5-7378c12c5784",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> Before initiating the model training we may have to execute different data pre-processing steps. Here we are demonstrating the scaling of VV and VH variable by using Standard Scaler.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d859a23d-be08-4a7c-b27b-1a4afb03feb5",
   "metadata": {},
   "source": [
    "<p align = \"justify\">Feature Scaling is a data preprocessing step for numerical features. Many machine learning algorithms like Gradient descent methods, KNN algorithm, linear and logistic regression, etc. require data scaling to produce good results. Scikit learn provides functions that can be used to apply data scaling. Here we are using Standard Scaler.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85328e-bef5-4b81-b7a0-443557387e0c",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 4 </strong></h4>\n",
    "<p align=\"justify\">Participants might explore other feature scaling techniques like Min Max Scaler, Max Absolute Scaling, Robust Scaling etc.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506aede-aca0-4acc-a9b7-c7547dd9ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1de27d-2b8b-4a3d-bf83-66f69f435402",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e978bb-a1af-48d7-ae11-6912bbe45adf",
   "metadata": {},
   "source": [
    "<p justify =\"align\">Now that we have the data in a format appropriate for machine learning, we can begin training a model. In this demonstration notebook, we have used a binary logistic regression model from the scikit-learn library. This library offers a wide range of other models, each with the capacity for extensive parameter tuning and customization capabilities.</p>\n",
    "\n",
    "<p justify =\"align\">Scikit-learn models require separation of predictor variables and the response variable. You have to store the predictor variables in array X and the response variable in the array Y. You must make sure not to include the response variable in array X. It also doesn't make sense to use latitude and longitude as predictor variables in such a confined area, so we drop those too.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f7099-afbf-47d5-a214-00ae0e11bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f54ac-ccd9-43a0-a53c-3403ee56c43d",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c940c3e9-caa0-4081-9941-372082cad66b",
   "metadata": {},
   "source": [
    "Now that we have trained our model , all that is left is to evaluate it. For evaluation we will generate the classification report and will plot the confusion matrix. Scikit-learn provides many other metrics that can be used for evaluation. You can even write a code on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad0925-b550-4b26-ae96-2ec160ae3dff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In-Sample Evaluation\n",
    "<p align=\"Jutisfy\"> We will be generating a classification report and a confusion matrix for the training data. It must be stressed that this is in-sample performance testing , which is the performance testing on the training dataset. These metrics are NOT truly indicative of the model's performance. You should wait to test the model performance on the test data before you feel confident about your model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab57db4-c7e8-4ff2-ac31-eea3f7eab64f",
   "metadata": {},
   "source": [
    "In this section, we make predictions on the training set and store them in the <b><i>insample_ predictions</i></b> variable. A confusion matrix is generated to gauge the robustness of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4196f-226e-4c52-8a64-bc00535bc97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_predictions = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131de89-a22d-4912-9f9b-6c7a12769c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Insample Accuracy {0:.2f}%\".format(100*accuracy_score(insample_predictions,y_train)))\n",
    "print(classification_report(insample_predictions,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847aa72-37b8-498b-bd63-4130d1366b7e",
   "metadata": {},
   "source": [
    "<p> For plotting a confusion matrix we define the function <b><i>plot_confusion_matrix</i></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97e054-6e73-4d90-89e2-36ce094721f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_value,predicted_value,title,labels):\n",
    "    '''\n",
    "    Plots a confusion matrix.\n",
    "    Attributes:\n",
    "    true_value - The ground truth value for comparision.\n",
    "    predicted_value - The values predicted by the model.\n",
    "    title - Title of the plot.\n",
    "    labels - The x and y labels of the plot.\n",
    "    '''\n",
    "    cm = confusion_matrix(true_value,predicted_value)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax, cmap='Blues');\n",
    "    ax.set_xlabel('Predicted labels');\n",
    "    ax.set_ylabel('True labels'); \n",
    "    ax.set_title(title); \n",
    "    ax.xaxis.set_ticklabels(labels); \n",
    "    ax.yaxis.set_ticklabels(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c77326-b7d5-4dc4-8101-96bece8aceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_train,insample_predictions,\"Model Level 1: Logistic\\nRegression Model In-Sample Results\",['Rice', 'Non Rice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd9f2d-1add-499a-b692-f616ec02871d",
   "metadata": {},
   "source": [
    "### Out-Sample Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e1681-0cae-42c1-ad00-0ad3bb7eca71",
   "metadata": {},
   "source": [
    "When evaluating a machine learning model, it is essential to correctly and fairly evaluate the model's ability to generalize. This is because models have a tendency to overfit the dataset they are trained on. To estimate the out-of-sample performance, we will predict on the test data now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7324d1-35fd-463f-9734-f2ad1552ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "outsample_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdec320-a85c-49b9-8177-a65d5b85317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy {0:.2f}%\".format(100*accuracy_score(outsample_predictions, y_test)))\n",
    "print(classification_report(y_test, outsample_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ed827-f3b8-47a2-865c-61bd2cd9621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, outsample_predictions,\"Model Level 1: Logistic\\nRegression Model Out-Sample Results\",['Rice', 'Non Rice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0bd4a4-5976-4a37-8f5b-fe46cf841e1f",
   "metadata": {},
   "source": [
    "From the above, we see that the model is able to achieve an F1 score of <b>0.57</b>. This is not a very good score, so your goal is to improve this score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629f3f5-5a51-45d7-9c68-75b969101405",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c178f-1113-4456-b1c6-3188273bc1f3",
   "metadata": {},
   "source": [
    "Once you are happy with your model, you can make a submission. To make a submission, you will need to use your model to make predictions about the presence of rice crops for a set of test coordinates we have provided in the <a href=\"https://challenge.ey.com/api/v1/storage/admin-files/6847912254281276-63ca8b5ab12e510013520e2b-challenge_1_submission_template.csv\"><b>\"challenge_1_submission_template.csv\"</b></a> file and upload the file onto the challenge platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc4682-5a87-4ccb-9b4d-877221e397e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the coordinates for the submission\n",
    "test_file = pd.read_csv('challenge_1_submission_template.csv')\n",
    "test_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5e29a-3bfa-4f5e-ac00-ffbf6b18033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get Sentinel-1-RTC Data\n",
    "# time_slice = \"2020-03-20/2020-03-21\"\n",
    "# assests = ['vh','vv']\n",
    "# vh_vv = []\n",
    "# for coordinates in tqdm(test_file['Latitude and Longitude']):\n",
    "#     vh_vv.append(get_sentinel_data(coordinates,time_slice,assests))\n",
    "# submission_vh_vv_data = pd.DataFrame(vh_vv,columns =['vh','vv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b02ce9-ef4b-4e96-b895-cabca85f2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_vh_vv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb45aca-974a-41e1-a522-36ca202d9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling \n",
    "submission_vh_vv_data = submission_vh_vv_data.values\n",
    "transformed_submission_data = sc.transform(submission_vh_vv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc34e3a-bfc1-49a0-94bb-ee1e540abe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "final_predictions = model.predict(transformed_submission_data)\n",
    "final_prediction_series = pd.Series(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375e4cd-fdb0-4b82-8508-6ff022061cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the results into dataframe\n",
    "submission_df = pd.DataFrame({'Latitude and Longitude':test_file['Latitude and Longitude'].values, 'Class of Land':final_prediction_series.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a563b-c0ed-4e2c-a7fe-08ad59cf1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the sample submission dataframe\n",
    "display(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcec864-4da3-41e7-80da-b77e42a9d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping the predictions into a csv file.\n",
    "submission_df.to_csv(\"challenge_1_submission_rice_crop_prediction.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c2fba-5ecb-4ea5-a4a5-1fd6a74952cc",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10632df0-a001-4932-8ef9-c7dc9023cc7f",
   "metadata": {},
   "source": [
    "Now that you have learned a basic approach to model training, it’s time to try your own approach! Feel free to modify any of the functions presented in this notebook. We look forward to seeing your version of the model and the results. Best of luck with the challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
